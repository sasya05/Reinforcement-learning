{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJ1Hig9W5ZjLXoh3RAKh6R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sasya05/Reinforcement-learning/blob/main/lab8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gymnasium[classic-control] torch numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lGM0OnH_F-k",
        "outputId": "7a841a56-5960-4a98-fa9f-b95d7a5d7611"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: gymnasium[classic-control] in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (0.0.4)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (2.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# ✅ Advantage Actor–Critic (A2C) — Continuous Action Example\n",
        "# Works with Python 3.12, NumPy ≥ 2.0, Gymnasium, and PyTorch\n",
        "# =========================================================\n",
        "\n",
        "import numpy as np\n",
        "if not hasattr(np, 'bool8'):\n",
        "    np.bool8 = np.bool_   # Compatibility for NumPy ≥ 2.0\n",
        "\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "import time\n",
        "\n",
        "# ------------------------------\n",
        "# Actor–Critic Network\n",
        "# ------------------------------\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        # Shared base\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Actor outputs mean\n",
        "        self.mu = nn.Linear(hidden_dim, action_dim)\n",
        "        # Learnable log std\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
        "        # Critic outputs value\n",
        "        self.value = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.shared(state)\n",
        "        mu = self.mu(x)\n",
        "        std = torch.exp(self.log_std)\n",
        "        value = self.value(x)\n",
        "        return mu, std, value\n",
        "\n",
        "# ------------------------------\n",
        "# A2C Agent\n",
        "# ------------------------------\n",
        "class A2CAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99):\n",
        "        self.gamma = gamma\n",
        "        self.model = ActorCritic(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        mu, std, _ = self.model(state)\n",
        "        dist = Normal(mu, std)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
        "        return action.squeeze(0).detach().numpy(), log_prob\n",
        "\n",
        "    def compute_returns(self, rewards, dones, values, next_value):\n",
        "        returns = []\n",
        "        R = next_value\n",
        "        for r, d in zip(reversed(rewards), reversed(dones)):\n",
        "            R = r + self.gamma * R * (1 - d)\n",
        "            returns.insert(0, R)\n",
        "        return returns\n",
        "\n",
        "    def update(self, log_probs, values, rewards, dones, next_value):\n",
        "        returns = self.compute_returns(rewards, dones, values, next_value)\n",
        "        returns = torch.cat(returns).detach()\n",
        "        values = torch.cat(values)\n",
        "        log_probs = torch.cat(log_probs)\n",
        "\n",
        "        advantage = returns - values\n",
        "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
        "        critic_loss = advantage.pow(2).mean()\n",
        "        loss = actor_loss + 0.5 * critic_loss\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "# ------------------------------\n",
        "# Training Loop\n",
        "# ------------------------------\n",
        "def train(env_name=\"Pendulum-v1\", num_episodes=300, max_steps=200):\n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "\n",
        "    agent = A2CAgent(state_dim, action_dim)\n",
        "    rewards_all = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        log_probs, values, rewards, dones = [], [], [], []\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            action, log_prob = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "            _, _, value = agent.model(state_tensor)\n",
        "\n",
        "            log_probs.append(log_prob.unsqueeze(0))\n",
        "            values.append(value)\n",
        "            rewards.append(torch.tensor([reward], dtype=torch.float32))\n",
        "            dones.append(torch.tensor([done], dtype=torch.float32))\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        next_state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        _, _, next_value = agent.model(next_state_tensor)\n",
        "\n",
        "        agent.update(log_probs, values, rewards, dones, next_value)\n",
        "        rewards_all.append(total_reward)\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            avg_reward = np.mean(rewards_all[-10:])\n",
        "            print(f\"Episode {episode}, Average Reward: {avg_reward:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "    torch.save(agent.model.state_dict(), \"a2c_policy.pth\")\n",
        "    print(\"✅ Training complete! Model saved as a2c_policy.pth\")\n",
        "    return agent, rewards_all\n",
        "\n",
        "# ------------------------------\n",
        "# Visualization / Playback\n",
        "# ------------------------------\n",
        "def visualize(agent, env_name=\"Pendulum-v1\", episodes=3):\n",
        "    env = gym.make(env_name, render_mode=\"human\")\n",
        "    for ep in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        for _ in range(200):\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "            mu, std, _ = agent.model(state_tensor)\n",
        "            dist = Normal(mu, std)\n",
        "            action = dist.mean.detach().numpy()[0]\n",
        "            state, reward, terminated, truncated, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "        print(f\"🎮 Episode {ep+1}: Reward = {total_reward:.2f}\")\n",
        "    env.close()\n",
        "\n",
        "# ------------------------------\n",
        "# Main Entry\n",
        "# ------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    start = time.time()\n",
        "    agent, rewards = train(num_episodes=300)\n",
        "    print(f\"Training done in {round(time.time() - start, 2)} sec\")\n",
        "    visualize(agent)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siiV2f1K_L5G",
        "outputId": "b20e8f8f-7a54-475f-8430-3f9b1f39a425"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Average Reward: -1687.95\n",
            "Episode 10, Average Reward: -1436.97\n",
            "Episode 20, Average Reward: -1392.66\n",
            "Episode 30, Average Reward: -1475.79\n",
            "Episode 40, Average Reward: -1255.14\n",
            "Episode 50, Average Reward: -1481.63\n",
            "Episode 60, Average Reward: -1406.42\n",
            "Episode 70, Average Reward: -1424.84\n",
            "Episode 80, Average Reward: -1498.36\n",
            "Episode 90, Average Reward: -1341.39\n",
            "Episode 100, Average Reward: -1336.14\n",
            "Episode 110, Average Reward: -1338.94\n",
            "Episode 120, Average Reward: -1337.62\n",
            "Episode 130, Average Reward: -1354.38\n",
            "Episode 140, Average Reward: -1402.93\n",
            "Episode 150, Average Reward: -1376.94\n",
            "Episode 160, Average Reward: -1385.53\n",
            "Episode 170, Average Reward: -1365.98\n",
            "Episode 180, Average Reward: -1234.99\n",
            "Episode 190, Average Reward: -1363.01\n",
            "Episode 200, Average Reward: -1352.72\n",
            "Episode 210, Average Reward: -1395.63\n",
            "Episode 220, Average Reward: -1332.62\n",
            "Episode 230, Average Reward: -1484.52\n",
            "Episode 240, Average Reward: -1395.51\n",
            "Episode 250, Average Reward: -1249.93\n",
            "Episode 260, Average Reward: -1376.67\n",
            "Episode 270, Average Reward: -1362.15\n",
            "Episode 280, Average Reward: -1417.90\n",
            "Episode 290, Average Reward: -1376.47\n",
            "✅ Training complete! Model saved as a2c_policy.pth\n",
            "Training done in 56.58 sec\n",
            "🎮 Episode 1: Reward = -1356.64\n",
            "🎮 Episode 2: Reward = -1646.94\n",
            "🎮 Episode 3: Reward = -1198.31\n"
          ]
        }
      ]
    }
  ]
}