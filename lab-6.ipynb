{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZfNopAQGRW8FUgShzUoxM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sasya05/Reinforcement-learning/blob/main/lab-6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# ✅ Deep Q-Network (DQN) Implementation — Final Version\n",
        "# Works with Python 3.12, NumPy ≥ 2.0, Gymnasium, and PyTorch\n",
        "# =========================================================\n",
        "\n",
        "import numpy as np\n",
        "if not hasattr(np, 'bool8'):\n",
        "    np.bool8 = np.bool_  # Compatibility for NumPy ≥ 2.0\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from collections import deque, namedtuple\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ------------------------------\n",
        "# DQN Network\n",
        "# ------------------------------\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# ------------------------------\n",
        "# Replay Buffer\n",
        "# ------------------------------\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.buffer.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# ------------------------------\n",
        "# Agent\n",
        "# ------------------------------\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, batch_size=64, eps_start=1.0, eps_end=0.01, eps_decay=500):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.eps_start = eps_start\n",
        "        self.eps_end = eps_end\n",
        "        self.eps_decay = eps_decay\n",
        "\n",
        "        self.policy_net = DQN(state_dim, action_dim)\n",
        "        self.target_net = DQN(state_dim, action_dim)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "\n",
        "        self.memory = ReplayBuffer()\n",
        "        self.steps_done = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * math.exp(-1. * self.steps_done / self.eps_decay)\n",
        "        self.steps_done += 1\n",
        "        if random.random() < eps_threshold:\n",
        "            return torch.tensor([[random.randrange(self.action_dim)]], dtype=torch.long)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
        "\n",
        "    def optimize(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "        transitions = self.memory.sample(self.batch_size)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "        next_state_batch = torch.cat(batch.next_state)\n",
        "        done_batch = torch.cat(batch.done)\n",
        "\n",
        "        q_values = self.policy_net(state_batch).gather(1, action_batch)\n",
        "        next_q_values = self.target_net(next_state_batch).max(1)[0].detach()\n",
        "        expected_q_values = reward_batch + (self.gamma * next_q_values * (1 - done_batch))\n",
        "\n",
        "        loss = F.smooth_l1_loss(q_values, expected_q_values.unsqueeze(1))\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "# ------------------------------\n",
        "# Training Loop\n",
        "# ------------------------------\n",
        "def train(env_name=\"CartPole-v1\", num_episodes=500):\n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    agent = DQNAgent(state_dim, action_dim)\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        state = torch.tensor([state], dtype=torch.float32)\n",
        "        total_reward = 0\n",
        "\n",
        "        for t in range(500):\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "\n",
        "            next_state_tensor = torch.tensor([next_state], dtype=torch.float32)\n",
        "            reward_tensor = torch.tensor([reward], dtype=torch.float32)\n",
        "            done_tensor = torch.tensor([float(done)], dtype=torch.float32)\n",
        "\n",
        "            agent.memory.push(state, action, reward_tensor, next_state_tensor, done_tensor)\n",
        "            state = next_state_tensor\n",
        "\n",
        "            agent.optimize()\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "        agent.update_target()\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            print(f\"Episode {episode}, Reward: {total_reward:.1f}\")\n",
        "\n",
        "    env.close()\n",
        "    torch.save(agent.policy_net.state_dict(), \"dqn_policy.pth\")\n",
        "    print(\"✅ Training complete! Model saved as dqn_policy.pth\")\n",
        "    return agent, rewards\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start = time.time()\n",
        "    agent, rewards = train(num_episodes=200)\n",
        "    print(\"Training done in\", round(time.time() - start, 2), \"seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSNCyhzQ9VLE",
        "outputId": "400542d5-5303-4139-8f6f-d04b000307cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-347561982.py:119: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  state = torch.tensor([state], dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Reward: 16.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10, Reward: 13.0\n",
            "Episode 20, Reward: 11.0\n",
            "Episode 30, Reward: 10.0\n",
            "Episode 40, Reward: 10.0\n",
            "Episode 50, Reward: 11.0\n",
            "Episode 60, Reward: 8.0\n",
            "Episode 70, Reward: 12.0\n",
            "Episode 80, Reward: 8.0\n",
            "Episode 90, Reward: 10.0\n",
            "Episode 100, Reward: 8.0\n",
            "Episode 110, Reward: 10.0\n",
            "Episode 120, Reward: 10.0\n",
            "Episode 130, Reward: 9.0\n",
            "Episode 140, Reward: 9.0\n",
            "Episode 150, Reward: 10.0\n",
            "Episode 160, Reward: 10.0\n",
            "Episode 170, Reward: 10.0\n",
            "Episode 180, Reward: 9.0\n",
            "Episode 190, Reward: 10.0\n",
            "✅ Training complete! Model saved as dqn_policy.pth\n",
            "Training done in 7.73 seconds\n"
          ]
        }
      ]
    }
  ]
}